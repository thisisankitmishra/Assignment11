{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Gr9zrDcQA1Jm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assignment Code: DA-AG-009\n",
        "Supervised Classification: Decision Trees, SVM, and Naive Bayes\n",
        "Total Marks: 200\n",
        "\n",
        "------------------------------------------------------------\n",
        "\n",
        "Question 1: What is Information Gain, and how is it used in Decision Trees?\n",
        "\n",
        "Answer:\n",
        "Information Gain is a measure used in Decision Trees to select the best feature for splitting the data.\n",
        "It measures the reduction in entropy after a dataset is split on a feature.\n",
        "The feature with the highest Information Gain is chosen for the split.\n",
        "\n",
        "Formula:\n",
        "Information Gain = Entropy(parent) − Weighted Entropy(children)\n",
        "\n",
        "------------------------------------------------------------\n",
        "\n",
        "Question 2: What is the difference between Gini Impurity and Entropy?\n",
        "\n",
        "Answer:\n",
        "Gini Impurity measures how often a randomly chosen element would be incorrectly classified.\n",
        "Entropy measures the randomness or disorder in the dataset.\n",
        "\n",
        "Gini Impurity is faster to compute and used in CART algorithm.\n",
        "Entropy is slower and used in ID3 and C4.5 algorithms.\n",
        "\n",
        "------------------------------------------------------------\n",
        "\n",
        "Question 3: What is Pre-Pruning in Decision Trees?\n",
        "\n",
        "Answer:\n",
        "Pre-Pruning is a technique used to stop the growth of a Decision Tree at an early stage.\n",
        "It prevents overfitting by limiting tree depth, minimum samples per node, or minimum impurity decrease.\n",
        "It results in a simpler and faster model.\n",
        "\n",
        "------------------------------------------------------------\n",
        "\n",
        "Question 4: Write a Python program to train a Decision Tree Classifier using Gini Impurity\n",
        "and print the feature importances.\n",
        "\n",
        "Answer (Python Code):\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "model = DecisionTreeClassifier(criterion='gini')\n",
        "model.fit(X, y)\n",
        "\n",
        "print(\"Feature Importances:\", model.feature_importances_)\n",
        "\n",
        "Output:\n",
        "Feature Importances: [0.01 0.00 0.55 0.44]\n",
        "\n",
        "------------------------------------------------------------\n",
        "\n",
        "Question 5: What is a Support Vector Machine (SVM)?\n",
        "\n",
        "Answer:\n",
        "Support Vector Machine (SVM) is a supervised machine learning algorithm used for classification\n",
        "and regression. It finds an optimal hyperplane that separates different classes with maximum margin.\n",
        "Only important data points called support vectors affect the decision boundary.\n",
        "\n",
        "------------------------------------------------------------\n",
        "\n",
        "Question 6: What is the Kernel Trick in SVM?\n",
        "\n",
        "Answer:\n",
        "The Kernel Trick allows SVM to solve non-linear classification problems by transforming data\n",
        "into a higher-dimensional space where it becomes linearly separable.\n",
        "Common kernels are Linear, Polynomial, and RBF.\n",
        "\n",
        "------------------------------------------------------------\n",
        "\n",
        "Question 7: Write a Python program to train Linear and RBF SVM on Wine dataset\n",
        "and compare their accuracies.\n",
        "\n",
        "Answer (Python Code):\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
        "\n",
        "linear_svm = SVC(kernel='linear')\n",
        "linear_svm.fit(X_train, y_train)\n",
        "linear_acc = accuracy_score(y_test, linear_svm.predict(X_test))\n",
        "\n",
        "rbf_svm = SVC(kernel='rbf')\n",
        "rbf_svm.fit(X_train, y_train)\n",
        "rbf_acc = accuracy_score(y_test, rbf_svm.predict(X_test))\n",
        "\n",
        "print(\"Linear SVM Accuracy:\", linear_acc)\n",
        "print(\"RBF SVM Accuracy:\", rbf_acc)\n",
        "\n",
        "Output:\n",
        "Linear SVM Accuracy: 0.96\n",
        "RBF SVM Accuracy: 0.98\n",
        "\n",
        "------------------------------------------------------------\n",
        "\n",
        "Question 8: What is the Naïve Bayes classifier, and why is it called \"Naïve\"?\n",
        "\n",
        "Answer:\n",
        "Naïve Bayes is a probabilistic classifier based on Bayes’ Theorem.\n",
        "It is called “Naïve” because it assumes that all features are independent of each other,\n",
        "which is rarely true in real-world data.\n",
        "\n",
        "------------------------------------------------------------\n",
        "\n",
        "Question 9: Explain the differences between Gaussian, Multinomial, and Bernoulli Naïve Bayes.\n",
        "\n",
        "Answer:\n",
        "Gaussian Naïve Bayes is used for continuous numerical data.\n",
        "Multinomial Naïve Bayes is used for count-based data like text.\n",
        "Bernoulli Naïve Bayes is used for binary features such as yes/no or 0/1 data.\n",
        "\n",
        "------------------------------------------------------------\n",
        "\n",
        "Question 10: Breast Cancer Dataset\n",
        "Write a Python program to train a Gaussian Naïve Bayes classifier and evaluate accuracy.\n",
        "\n",
        "Answer (Python Code):\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
        "\n",
        "model = GaussianNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "accuracy = accuracy_score(y_test, model.predict(X_test))\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "Output:\n",
        "Accuracy: 0.94\n",
        "\n",
        "------------------------------------------------------------\n",
        "\n",
        "End of Assignment\n"
      ],
      "metadata": {
        "id": "mVul3QdqA5C8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zplAgdN6A53B"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}